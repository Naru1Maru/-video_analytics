# Video Pipelines & Temporal Segmentation Stability

Комплексный проект, посвящённый **инженерным аспектам обработки видео** в задачах компьютерного зрения.
Репозиторий состоит из двух логически связанных частей:

* **HW1** — оптимизация и профилирование видеопайплайна;
* **HW2** — временная стабилизация масок сегментации на видео.

---

## Содержание

* [HW1 — Video Pipeline Optimization](#hw1--video-pipeline-optimization)
* [HW2 — Temporal Mask Stabilization for Video Segmentation](#hw2--temporal-mask-stabilization-for-video-segmentation)
* [Требования](#требования)
* [Как использовать](#как-использовать)

---

## HW1 — Video Pipeline Optimization

<a name="hw1--video-pipeline-optimization"></a>

### Описание

HW1 посвящена ускорению и анализу видеопайплайна для задач компьютерного зрения.
Основной фокус — **производительность**, **latency**, **throughput** и **стабильность обработки**.

Проект включает:

* CPU/GPU декодирование видео;
* препроцессинг;
* DataLoader-параллелизм;
* overlap decode ↔ inference;
* near-real-time обработку видеопотока.

---

### Цели HW1

* Проанализировать узкие места пайплайна (decode → preprocess → infer).
* Оценить влияние `num_workers`, `pin_memory`, `prefetch_factor`.
* Сравнить CPU и GPU препроцессинг.
* Реализовать overlap-пайплайн.
* Построить near-real-time режим.
* Сравнить offline и streaming режимы обработки.

---

### Структура HW1

#### **`HW1.ipynb` — основной ноутбук**

Содержит полный разбор задач:

* чтение видеокадров и построение датасета;
* влияние числа воркеров;
* профилирование (Torch Profiler);
* PyAV vs decord;
* CPU vs GPU препроцессинг;
* overlap decode ↔ infer;
* near-real-time pipeline;
* сравнение offline vs streaming.

В ноутбуке представлены графики, таблицы и подробные выводы.

---

#### **`video_pipeline_ucf101.py` — реализованный видеопайплайн**

Python-модуль с полной реализацией:

* `read_clip` через PyAV;
* `VideoDataset` + DataLoader;
* функции профилирования;
* декодеры PyAV / decord;
* CPU/GPU препроцессинг;
* overlap-пайплайн с CUDA Stream;
* near-real-time обработка.

Файл может использоваться как библиотека для повторного запуска экспериментов.

---

### Основные результаты HW1

* Ускорение DataLoader от `num_workers=0` → `4` более чем в **60 раз**.
* GPU-препроцессинг быстрее CPU более чем в **1600 раз**.
* Overlap-пайплайн снижает latency до ~**2 ms**.
* Near-real-time режим обеспечивает стабильную обработку с низким jitter (~**0.3 ms**).
* Offline режим достигает >**16000 FPS**, но менее стабилен.
* PyAV быстрее decord на CPU, decord выигрывает при GPU-декодировании.

---

## HW2 — Temporal Mask Stabilization for Video Segmentation

<a name="hw2--temporal-mask-stabilization-for-video-segmentation"></a>

### Описание

HW2 посвящена **качеству сегментации на видео во времени**.
Проблема: покадровая сегментация приводит к мерцанию масок, скачкам площади и «залипанию» после выхода объекта из кадра.

Цель HW2 — **уменьшить временную нестабильность масок без переобучения модели**, используя только постобработку видеопоследовательности.

---

### Структура HW2

#### **`HW2.ipynb` — временная стабилизация масок**

Ноутбук включает:

* покадровую instance-сегментацию;
* перенос маски по оптическому потоку;
* motion-weighted fusion;
* экспоненциальное сглаживание (EMA);
* адаптивную бинаризацию;
* контроль резких изменений площади;
* авто-исчезновение объекта (anti-ghost);
* автоматический подбор параметров;
* количественную и визуальную оценку.

---

### Использованная модель

* **Mask R-CNN (ResNet-50-FPN, torchvision, pretrained COCO)**

Выбор обусловлен:

* instance segmentation;
* наличием confidence score;
* стабильной реализацией;
* воспроизводимостью без обучения.

---

### Схема пайплайна стабилизации

1. Покадровая сегментация.
2. Перенос маски по оптическому потоку.
3. Взвешенное объединение с текущим предсказанием.
4. EMA-сглаживание вероятностных карт.
5. Адаптивная бинаризация по площади.
6. Anti-ghost логика (EWMA confidence + fade-out).

---

### Визуальные примеры

* видео **raw vs stabilized**;
* покадровые маски;
* примеры корректного переноса;
* примеры провалов сегментации;
* график IoU между кадрами;
* график площади маски;
* график адаптивного порога;
* графики confidence и miss-streak.

---

### Основные результаты HW2

* Mean IoU: **0.78 → 0.93**
* Mean L1: снижение более чем в **5 раз**
* Area CV: **1.09 → 0.75**
* Устранено «залипание» маски после выхода объекта.

---

### Технический вывод HW2

Мерцание масок мешает трекингу, анализу движения и измерению площади объектов.
Перенос маски и временное сглаживание значительно повышают устойчивость сегментации, однако требуют аккуратного баланса, так как чрезмерное сглаживание может ухудшать форму маски при низкой уверенности модели.

---

## Требования

<a name="требования"></a>

* Python 3.9+
* PyTorch
* torchvision
* OpenCV
* PyAV
* decord (опционально)
* Jupyter Notebook

---

## Как использовать

<a name="как-использовать"></a>

1. Открыть `HW1.ipynb` для анализа видеопайплайна и производительности.
2. Открыть `HW2.ipynb` для воспроизведения экспериментов по стабилизации масок.
3. Использовать Python-модуль и ноутбуки как основу для собственных экспериментов с видео.

